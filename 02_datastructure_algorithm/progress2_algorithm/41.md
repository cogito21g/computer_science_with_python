### 10. 고급 동적 계획법 

#### 10.4 확률적 동적 계획법 (Probabilistic Dynamic Programming)

#### 10.4.1 마르코프 결정 프로세스 (Markov Decision Process, MDP)

#### 이론
**마르코프 결정 프로세스 (Markov Decision Process, MDP)**: MDP는 의사결정 문제를 모델링하기 위한 수학적 틀입니다. MDP는 다음 네 가지 요소로 구성됩니다.
- **상태 (State, S)**: 시스템이 있을 수 있는 모든 상태의 집합
- **행동 (Action, A)**: 각 상태에서 취할 수 있는 행동의 집합
- **전이 확률 (Transition Probability, P)**: 상태-행동 쌍에서 다음 상태로의 전이 확률
- **보상 (Reward, R)**: 상태-행동 쌍에 대해 얻는 보상

MDP의 목표는 주어진 상태에서 출발하여 기대되는 총 보상을 최대화하는 정책(Policy)을 찾는 것입니다.

#### 벨만 방정식 (Bellman Equation)
벨만 방정식은 최적 정책을 찾기 위한 기본적인 도구입니다. 벨만 방정식은 다음과 같습니다:
\[ V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right) \]
여기서 \( V(s) \)는 상태 \( s \)의 가치(Value)를 나타내고, \( \gamma \)는 할인 인자(Discount Factor)입니다.

#### 구현
MDP를 동적 계획법으로 해결하는 방법을 Python으로 구현해보겠습니다. 예시로 그리드 월드(Grid World) 환경을 사용합니다.

```python
import numpy as np

class GridWorldMDP:
    def __init__(self, grid_size, terminal_states, obstacles, rewards, gamma=0.9):
        self.grid_size = grid_size
        self.terminal_states = terminal_states
        self.obstacles = obstacles
        self.rewards = rewards
        self.gamma = gamma
        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        self.value_function = np.zeros(grid_size)
        self.policy = np.zeros(grid_size, dtype=(int, 2))

    def is_terminal(self, state):
        return state in self.terminal_states

    def is_obstacle(self, state):
        return state in self.obstacles

    def get_next_states(self, state):
        next_states = []
        for action in self.actions:
            next_state = (state[0] + action[0], state[1] + action[1])
            if 0 <= next_state[0] < self.grid_size[0] and 0 <= next_state[1] < self.grid_size[1] and not self.is_obstacle(next_state):
                next_states.append(next_state)
            else:
                next_states.append(state)
        return next_states

    def value_iteration(self, theta=1e-6):
        while True:
            delta = 0
            new_value_function = self.value_function.copy()
            for i in range(self.grid_size[0]):
                for j in range(self.grid_size[1]):
                    state = (i, j)
                    if self.is_terminal(state) or self.is_obstacle(state):
                        continue
                    v = self.value_function[state]
                    new_value = float('-inf')
                    for action in self.actions:
                        next_state = (state[0] + action[0], state[1] + action[1])
                        if 0 <= next_state[0] < self.grid_size[0] and 0 <= next_state[1] < self.grid_size[1] and not self.is_obstacle(next_state):
                            reward = self.rewards.get(next_state, 0)
                            new_value = max(new_value, reward + self.gamma * self.value_function[next_state])
                    new_value_function[state] = new_value
                    delta = max(delta, abs(v - new_value))
            self.value_function = new_value_function
            if delta < theta:
                break

    def extract_policy(self):
        for i in range(self.grid_size[0]):
            for j in range(self.grid_size[1]):
                state = (i, j)
                if self.is_terminal(state) or self.is_obstacle(state):
                    continue
                best_action = None
                best_value = float('-inf')
                for action in self.actions:
                    next_state = (state[0] + action[0], state[1] + action[1])
                    if 0 <= next_state[0] < self.grid_size[0] and 0 <= next_state[1] < self.grid_size[1] and not self.is_obstacle(next_state):
                        reward = self.rewards.get(next_state, 0)
                        value = reward + self.gamma * self.value_function[next_state]
                        if value > best_value:
                            best_value = value
                            best_action = action
                self.policy[state] = best_action

# 예시 그리드 월드 설정
grid_size = (4, 4)
terminal_states = [(3, 3)]
obstacles = [(1, 1)]
rewards = { (3, 3): 1, (3, 2): -1 }
gamma = 0.9

mdp = GridWorldMDP(grid_size, terminal_states, obstacles, rewards, gamma)
mdp.value_iteration()
mdp.extract_policy()

print("Value Function:")
print(mdp.value_function)
print("Policy:")
print(mdp.policy)
```

이 코드에서 `GridWorldMDP` 클래스는 그리드 월드 환경에서 MDP를 나타내며, 가치 반복(Value Iteration) 알고리즘을 사용하여 최적 정책을 찾습니다. `value_iteration` 메서드는 벨만 방정식을 사용하여 가치 함수를 반복적으로 계산하고, `extract_policy` 메서드는 최적 정책을 추출합니다.

### 실습
다음 코드를 직접 실행해보고, 다른 그리드 월드 설정에 대해서도 테스트해 보세요.

```python
# 또 다른 예시 그리드 월드 설정
grid_size = (3, 3)
terminal_states = [(2, 2)]
obstacles = [(1, 1)]
rewards = { (2, 2): 1, (2, 1): -1 }
gamma = 0.8

mdp = GridWorldMDP(grid_size, terminal_states, obstacles, rewards, gamma)
mdp.value_iteration()
mdp.extract_policy()

print("Value Function:")
print(mdp.value_function)
print("Policy:")
print(mdp.policy)
```
