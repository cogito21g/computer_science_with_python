### 10. 고급 동적 계획법 - 확률적 동적 계획법 (Probabilistic Dynamic Programming) - 강화 학습 기초 (Basic Reinforcement Learning)

#### 이론
**강화 학습 (Reinforcement Learning, RL)**: 강화 학습은 에이전트가 환경과 상호작용하면서 최적의 정책을 학습하는 방법입니다. 에이전트는 상태(state)를 관찰하고, 행동(action)을 선택하며, 보상(reward)을 받습니다. 목표는 누적 보상을 최대화하는 정책을 학습하는 것입니다.

- **주요 요소**:
  1. **상태 (State, S)**: 에이전트가 관찰하는 환경의 상태.
  2. **행동 (Action, A)**: 에이전트가 취할 수 있는 행동.
  3. **보상 (Reward, R)**: 행동의 결과로 받는 피드백.
  4. **정책 (Policy, π)**: 상태에서 행동을 선택하는 전략.
  5. **가치 함수 (Value Function, V)**: 상태의 가치.
  6. **Q-값 (Q-Value)**: 상태-행동 쌍의 가치.

- **Q-러닝 (Q-Learning)**: Q-러닝은 모델 프리 강화 학습 알고리즘 중 하나입니다. 에이전트가 환경과 상호작용하면서 상태-행동 쌍의 가치를 학습합니다. Q-러닝은 다음과 같은 업데이트 방식을 사용합니다:
\[ Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right) \]
여기서 \( \alpha \)는 학습률, \( \gamma \)는 할인 인자입니다.

#### 구현
Q-러닝을 Python으로 구현해보겠습니다. 예시로 그리드 월드(Grid World) 환경을 사용합니다.

```python
import numpy as np
import random

class GridWorldQLearning:
    def __init__(self, grid_size, terminal_states, obstacles, rewards, gamma=0.9, alpha=0.1, epsilon=0.1):
        self.grid_size = grid_size
        self.terminal_states = terminal_states
        self.obstacles = obstacles
        self.rewards = rewards
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        self.q_table = np.zeros((grid_size[0], grid_size[1], len(self.actions)))

    def is_terminal(self, state):
        return state in self.terminal_states

    def is_obstacle(self, state):
        return state in self.obstacles

    def get_next_state(self, state, action):
        next_state = (state[0] + action[0], state[1] + action[1])
        if 0 <= next_state[0] < self.grid_size[0] and 0 <= next_state[1] < self.grid_size[1] and not self.is_obstacle(next_state):
            return next_state
        return state

    def choose_action(self, state):
        if random.uniform(0, 1) < self.epsilon:
            return random.choice(range(len(self.actions)))
        else:
            return np.argmax(self.q_table[state[0], state[1]])

    def update_q_table(self, state, action, reward, next_state):
        best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])
        td_target = reward + self.gamma * self.q_table[next_state[0], next_state[1], best_next_action]
        td_delta = td_target - self.q_table[state[0], state[1], action]
        self.q_table[state[0], state[1], action] += self.alpha * td_delta

    def train(self, episodes):
        for _ in range(episodes):
            state = (random.randint(0, self.grid_size[0] - 1), random.randint(0, self.grid_size[1] - 1))
            while self.is_obstacle(state) or self.is_terminal(state):
                state = (random.randint(0, self.grid_size[0] - 1), random.randint(0, self.grid_size[1] - 1))

            while not self.is_terminal(state):
                action_idx = self.choose_action(state)
                action = self.actions[action_idx]
                next_state = self.get_next_state(state, action)
                reward = self.rewards.get(next_state, 0)
                self.update_q_table(state, action_idx, reward, next_state)
                state = next_state

    def get_policy(self):
        policy = np.zeros((self.grid_size[0], self.grid_size[1]), dtype=(int, 2))
        for i in range(self.grid_size[0]):
            for j in range(self.grid_size[1]):
                policy[i, j] = self.actions[np.argmax(self.q_table[i, j])]
        return policy

# 예시 그리드 월드 설정
grid_size = (4, 4)
terminal_states = [(3, 3)]
obstacles = [(1, 1)]
rewards = { (3, 3): 1, (3, 2): -1 }
gamma = 0.9
alpha = 0.1
epsilon = 0.1

q_learning = GridWorldQLearning(grid_size, terminal_states, obstacles, rewards, gamma, alpha, epsilon)
q_learning.train(episodes=1000)

print("Q-Table:")
print(q_learning.q_table)
print("Policy:")
print(q_learning.get_policy())
```

이 코드에서 `GridWorldQLearning` 클래스는 그리드 월드 환경에서 Q-러닝 알고리즘을 사용하여 최적 정책을 학습합니다. `train` 메서드는 여러 에피소드 동안 학습을 수행하며, `get_policy` 메서드는 학습된 정책을 반환합니다.

### 실습
다음 코드를 직접 실행해보고, 다른 그리드 월드 설정에 대해서도 테스트해 보세요.

```python
# 또 다른 예시 그리드 월드 설정
grid_size = (3, 3)
terminal_states = [(2, 2)]
obstacles = [(1, 1)]
rewards = { (2, 2): 1, (2, 1): -1 }
gamma = 0.8
alpha = 0.1
epsilon = 0.1

q_learning = GridWorldQLearning(grid_size, terminal_states, obstacles, rewards, gamma, alpha, epsilon)
q_learning.train(episodes=1000)

print("Q-Table:")
print(q_learning.q_table)
print("Policy:")
print(q_learning.get_policy())
```
